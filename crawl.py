import requests
from bs4 import BeautifulSoup
import urllib.parse

def crawl_website(url):
    # Make a GET request to the specified URL
    response = requests.get(url)
    
    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Find all the anchor tags in the HTML
    links = soup.find_all('a')
    
    # Loop through each link and process it
    for link in links:
        href = link.get('href')
        
        # Check if the link is a file
        if href and '.' in href:
            file_url = urllib.parse.urljoin(url, href)
            print("Found file:", file_url)
            # You can perform further actions with the file URL, such as downloading it
        
        # Check if the link is a relative URL
        elif href and not href.startswith('http'):
            absolute_url = urllib.parse.urljoin(url, href)
            crawl_website(absolute_url)

# Start crawling from a specified URL
crawl_website(input('Enter URL > '))
